{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "## Overview\n",
    "\n",
    "Suppose we have data and its scatter plot is shown on an image below. We need to classify the point (marked with a question mark). We can assume that it can belong to the same class as the nearest point. The problem here is the nearest point can be noise or outlier. In this case, we will get the wrong answer. To solve this issue, we first take K nearest points. In the example, the number of yellow points is more than others. So we can say our point most probably belongs to the yellow class. This method is the basic intuition behind the idea of K-nearest neighbours.\n",
    "\n",
    "<img src=\"img1.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "1. pick a value for K.\n",
    "2. calculate the distance from the new case hold out from each of the cases in the dataset\n",
    "3. search for the K-observations in the training data that are nearest to the measurements of the unknown data point.\n",
    "4. predict the response of the unknown data point using the most popular response value from the K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking a \"good\" value for K can be complicated. If we chose lower value the model can end-up by finding noise, or we can have overfitting. To high value can make our model unnecessarily generalized.\n",
    "\n",
    "One of the easy techniques to find a K is to iterate over some range of numbers and test our model for all Ks. Among those values pick the one with highest accuracy.\n",
    "\n",
    "<img src=\"img2.png\"/>\n",
    "\n",
    "We can use Minkowski distance to calculate the distance between two values $x$ and $x$:\n",
    "    \n",
    "$$D\\left(X,Y\\right)=\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}$$\n",
    "\n",
    "The Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance [Wikipedia]\n",
    "\n",
    "In our example we have use order 2, therefore we can re-write the formula as following:\n",
    "$$D(x_1, x_2) = \\sqrt{\\sum_{i=0}^{n}\\mid x_1i-x_2i  \\mid^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate the accuracy of the \"k-nearest neighbours\" model the Jaccard index can be used.\n",
    "\n",
    "$$ J(A,B) = \\frac{\\mid A \\cap B \\mid}{\\mid A \\cup B \\mid} = \\frac{\\mid A \\cap B \\mid}{\\mid A \\mid + \\mid B \\mid - \\mid A \\cap B \\mid} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
